[
  {
    "title": "FFT on GPU",
    "permalink": "https://elwhyjay.github.io/post/fft/",
    "date": "2026-02-07",
    "tags": ["FFT","Algorithm","GPU"],
    "content": "Introduction 이 포스트에서 다룰것은 Fast fourier Transform이다. 이 알고리즘에대해서는 PS(Competitve programming) 하는 사람들은 익히 알것이다. 실제로 검색을해보면 알고리즘에 대해서는 상세히 알수가있다. 그렇기때문에 알고리즘을 톺아보기보다 GPU위에서 FFT를 구현하는것에대해 초점을 맞춰 글을 작성할 예정이다.\nBackground Fourier Transform Fourier Transform(푸리에 변환)은 신호를 주파수 성분으로 분해하는 수학적 기법이다. 이는 시간 도메인에서 표현된 신호를 주파수 도메인으로 변환하여, 신호의 주파수 특성을 분석하고 처리하는 데 사용된다. 푸리에 변환은 다양한 분야에서 활용되며, 특히 신호 처리, 이미지 처리, 음성 인식, 통신 시스템 등에서 중요한 역할을 한다. 정의는 이러한데 그렇다면 푸리에 변환은 이런 특정 도메인이 아니면 전혀 쓸모가 없는것일까? 익히 알려져있듯 이러한 도메인이아니라 일반적인 연산 영역에서도 푸리에 변환은 유용하게 사용될 수 있다. 이를 중점으로 살펴보자.\n우선 푸리에 변환의 복잡한 수학적 정의는 생략하고 푸리에 변환이 주요 사용되는 지점들을 보면 신호와 같이 주기를 가지고 있는 데이터에서 유용하게 사용된다. 이런 주기성이 중요함을 인지하고 넘어가자.\nDiscrete Fourier Transform 실제 컴퓨터에서 다루는 데이터는 연속적인 신호가 아니라 이산적인 샘플로 표현된다. 따라서 푸리에 변환을 컴퓨터에서 구현하기 위해서는 이산 푸리에 변환(Discrete Fourier Transform, DFT)을 사용한다. DFT는 다음과 같이 정의된다: $$X_k = \\sum_{n=0}^{N-1} x_n \\cdot e^{-i \\frac{2\\pi}{N} kn}$$ 여기서:\n$X_k$: 주파수 도메인에서의 k번째 성분 $x_n$: 시간 도메인에서의 n번째 샘플 $N$: 샘플의 총 개수 $e^{-i \\frac{2\\pi}{N} kn}$: 복소 지수 함수로, 주파수 변환을 수행하는 역할 이때 $e^{-i \\frac{2\\pi}{N} kn}$는 오일러 공식에 의해 다음과 같이 표현될 수 있다: $$e^{-i \\frac{2\\pi}{N} kn} = \\cos\\left(\\frac{2\\pi}{N} kn\\right) - i \\sin\\left(\\frac{2\\pi}{N} kn\\right)$$\n이 수식에서 모든 $k$에 대해 모든 $n$을 순회하므로 DFT의 계산 복잡도는 $O(N^2)$이다. 샘플 수가 커질수록 계산 시간이 급격히 늘어나는데, 이를 줄이려면 수식 내부에 재활용 가능한 구조가 있어야 한다. $e^{-i\\frac{2\\pi}{N}kn}$ 항을 좀 더 자세히 들여다보자.\nPrimitive Root of Unity 복소 평면의 단위원($|z|=1$) 위에 $N$개의 점을 등간격으로 찍으면, 이 점들은 방정식 $z^N = 1$의 해가 된다. 이 해들 중 한 칸 회전에 해당하는 가장 기본적인 점을 primitive N-th root of unity라 하며 다음과 같이 정의한다: $$\\omega_N = e^{-i \\frac{2\\pi}{N}}$$\n나머지 점들은 $\\omega_N$을 거듭제곱하면 전부 얻을수 있다: $\\omega_N^0, \\omega_N^1, \\omega_N^2, \\ldots, \\omega_N^{N-1}$. 이 표기를 사용하면 DFT는 다음과 같이 간결하게 다시 쓸수 있다: $$X_k = \\sum_{n=0}^{N-1} x_n \\cdot \\omega_N^{kn}$$\n이게 단순히 표기를 줄인것에 불과하다면 굳이 도입할 이유가 없다. 핵심은 $\\omega_N$이 가지는 대수적 성질에 있다. FFT를 가능하게 하는 세 가지 성질을 살펴보자:\n주기성: $\\omega_N^N = 1$ — 지수가 $N$을 넘으면 다시 처음으로 돌아온다. 대칭성: $\\omega_N^{N/2} = -1$ — 단위원에서 정반대 점이므로 부호가 뒤집힌다. 따라서 $\\omega_N^{k+N/2} = -\\omega_N^k$이다. 축소(Halving): $\\omega_N^{2k} = \\omega_{N/2}^k$ — 제곱하면 $N$-th root가 $N/2$-th root로 바뀌어 문제 크기가 절반이 된다. 이 세 성질이 왜 중요한지는 FFT의 유도 과정에서 바로 드러난다.\nFast Fourier Transform FFT는 위 성질들을 활용하여 DFT의 $O(N^2)$를 $O(N \\log N)$으로 줄이는 알고리즘이다. 가장 널리 알려진 Cooley-Tukey 알고리즘을 따라가며 유도해보자.\nDFT 수식에서 인덱스 $n$을 짝수($2m$)와 홀수($2m+1$)로 나누면: $$X_k = \\sum_{m=0}^{N/2-1} x_{2m} \\cdot \\omega_N^{2mk} + \\sum_{m=0}^{N/2-1} x_{2m+1} \\cdot \\omega_N^{(2m+1)k}$$\n두 번째 항에서 $\\omega_N^k$를 밖으로 빼면: $$X_k = \\sum_{m=0}^{N/2-1} x_{2m} \\cdot \\omega_N^{2mk} + \\omega_N^k \\sum_{m=0}^{N/2-1} x_{2m+1} \\cdot \\omega_N^{2mk}$$\n여기서 축소 성질 $\\omega_N^{2mk} = \\omega_{N/2}^{mk}$를 적용하면: $$X_k = \\underbrace{\\sum_{m=0}^{N/2-1} x_{2m} \\cdot \\omega_{N/2}^{mk}}_{E_k} + \\omega_N^k \\underbrace{\\sum_{m=0}^{N/2-1} x_{2m+1} \\cdot \\omega_{N/2}^{mk}}_{O_k}$$\n$E_k$와 $O_k$는 각각 크기 $N/2$인 DFT이다. 그리고 대칭성 $\\omega_N^{k+N/2} = -\\omega_N^k$에 의해: $$X_{k+N/2} = E_k - \\omega_N^k \\cdot O_k$$\n즉 $E_k$와 $O_k$를 한 번 구하면 $X_k$와 $X_{k+N/2}$를 동시에 얻을수 있다. 이것이 butterfly 연산이다. 크기 $N$ 문제가 크기 $N/2$ 문제 2개 + $O(N)$ 결합으로 분할되므로: $$T(N) = 2T(N/2) + O(N) = O(N \\log N)$$\n$\\omega_N$의 성질 세 가지가 없었다면 이 분할은 불가능했을 것이다. 이제 이 알고리즘을 다항식 곱셈에 적용해보자.\n다항식 곱셈과 FFT 다항식의 두 가지 표현 다항식을 표현하는 방법에는 두 가지가 있다.\n계수 표현: 우리가 일반적으로 사용하는 형태이다. $$A(x) = a_0 + a_1 x + a_2 x^2 + \\cdots + a_{n-1} x^{n-1}$$\n점값 표현: 서로 다른 $N$개의 점에서의 함수값으로 다항식을 나타내는 방법이다. $${(x_0, A(x_0)),\\ (x_1, A(x_1)),\\ \\ldots,\\ (x_{N-1}, A(x_{N-1}))}$$\n$n-1$차 다항식은 $n$개의 서로 다른 점에서의 값으로 유일하게 결정되므로 두 표현은 동치이다.\n점값 표현이 강력한 이유는 곱셈에 있다. 두 다항식 $A(x)$, $B(x)$의 곱 $C(x) = A(x) \\cdot B(x)$를 점값 표현에서 구하면, 같은 점에서의 값을 그냥 곱하기만 하면 된다: $$C(x_k) = A(x_k) \\cdot B(x_k)$$\n이것은 $O(N)$이다. 반면 계수 표현에서의 직접 곱셈은: $$c_k = \\sum_{i=0}^{k} a_i \\cdot b_{k-i}$$\n이 convolution 연산은 $O(n^2)$이다. 그렇다면 전략은 명확하다.\n계수 표현 → 점값 표현 (변환) → 점별 곱셈 $O(N)$ → 점값 표현 → 계수 표현 (역변환)\n이 전략이 의미를 가지려면 변환과 역변환이 $O(n^2)$보다 빨라야 한다.\n왜 Root of Unity를 대입점으로 고르는가 임의의 $N$개 점에 다항식을 대입하면 각 점마다 $O(N)$, 점이 $N$개이므로 총 $O(N^2)$이다. 아무 점이나 골라서는 나아질게 없다.\n여기서 핵심적인 관찰이 등장한다. 다항식 $A(x)$에 $\\omega_N^k$를 대입하면: $$A(\\omega_N^k) = \\sum_{j=0}^{n-1} a_j \\cdot \\omega_N^{jk}$$\n이것은 계수 $a_j$에 대한 DFT와 정확히 같은 형태이다. 다시 말해 다항식을 roots of unity에서 평가하는 것이 곧 DFT이다. 다항식 자체에 주기성이 있어서가 아니다. 주기적 구조를 가진 점들을 대입점으로 선택한 것이고, 그 점들이 가진 대칭성과 축소 성질 덕분에 앞서 유도한 FFT를 그대로 적용하여 $O(N \\log N)$에 평가할수 있는것이다.\n역변환(점값 → 계수) 역시 깔끔하다: $$a_j = \\frac{1}{N} \\sum_{k=0}^{N-1} A(\\omega_N^k) \\cdot \\omega_N^{-jk}$$\n$\\omega_N$을 $\\omega_N^{-1}$로 바꾸고 $\\frac{1}{N}$을 곱한것 뿐이므로, 역변환도 FFT와 같은 구조로 $O(N \\log N)$에 수행할수 있다. 만약 임의의 점을 골랐다면 역변환은 Vandermonde 행렬의 역행렬을 구해야 하므로 이렇게 깔끔하게 떨어지지 않는다.\n전체 과정 정리하면 두 다항식 $A(x)$, $B(x)$의 곱셈은 다음과 같이 수행된다:\n$A(x)$와 $B(x)$의 계수 벡터에 FFT를 적용하여 점값 표현으로 변환 — $O(N \\log N)$ 변환된 점값을 요소별로 곱셈 — $O(N)$ 결과에 역 FFT(IFFT)를 적용하여 계수 표현으로 복원 — $O(N \\log N)$ 결국 두 다항식의 곱셈을 $O(N \\log N)$에 수행할수 있게 된다. 전체적인 흐름을 다이어그램으로 보면 다음과 같다.\nFFT on GPU 우선 naive한 CUDA 구현체는 다음과 같다\n#include \u0026lt;cuda_runtime.h\u0026gt; #include \u0026lt;cmath\u0026gt; #include \u0026lt;iostream\u0026gt; __global__ void copykernel(const float* input, float* output, int n) { int idx = blockDim.x * blockIdx.x + threadIdx.x; if(idx \u0026lt; 2*n) { output[idx] = input[idx]; } } __global__ void bitReverseKernel(float *input,int n, int lgn) { int idx = blockDim.x*blockIdx.x + threadIdx.x; if(idx \u0026lt; n) { int rev = 0; int tmp = idx; for(int i =0;i\u0026lt;lgn;i++) { rev = (rev\u0026lt;\u0026lt; 1) | (tmp \u0026amp; 1); tmp \u0026gt;\u0026gt;=1; } if(idx\u0026lt;rev){ float a0 = input[2*idx]; float a1 = input[2*idx+1]; float b0 = input[2*rev]; float b1 = input[2*rev+1]; input[2*idx] = b0; input[2*idx+1] = b1; input[2*rev] = a0; input[2*rev+1] = a1; } } } __global__ void fft_kernel(float* input,int n,int depth) { int half = 1 \u0026lt;\u0026lt; depth; int stride = half \u0026lt;\u0026lt; 1; int idx= blockDim.x * blockIdx.x + threadIdx.x; if (idx \u0026lt; n) { int pos = idx%stride; if (pos \u0026lt; half) { int i = idx; int j = half + i; float xr = input[2*i]; float xi = input[2*i+1]; float yr = input[2*j]; float yi = input[2*j+1]; float angle = M_PI * pos / half; float wr = cosf(angle); float wi = -sinf(angle); float tr = wr * yr - wi * yi; float ti = wi * yr + wr * yi; input[2*i] = xr + tr; input[2*i+1] = xi + ti; input[2*j] = xr - tr; input[2*j+1] = xi - ti; } } } __global__ void dft(const float* input, float* output ,int n) { int k = blockDim.x * blockIdx.x + threadIdx.x; if (k \u0026lt; n) { float sumRe = 0.0f; float sumIm = 0.0f; for (int t = 0; t \u0026lt; n; t++) { float angle = -2.0f * M_PI * k * t / n; float wr = cosf(angle); float wi = sinf(angle); float xr = input[2*t]; float xi = input[2*t + 1]; sumRe += xr * wr - xi * wi; sumIm += xr * wi + xi * wr; } output[2*k] = sumRe; output[2*k + 1] = sumIm; } } extern \u0026#34;C\u0026#34; void solve(const float* signal, float* spectrum, int N) { int lgn = 0; int tempN = N; while (tempN \u0026gt;\u0026gt;= 1) ++lgn; if((N \u0026amp; (N - 1)) != 0) { int threadsPerBlock = 256; int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock; dft\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(signal,spectrum, N); cudaDeviceSynchronize(); return; } else { int threadsPerBlock = 256; int blocksPerGrid = (2*N + threadsPerBlock - 1) / threadsPerBlock; copykernel\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(signal, spectrum, N); cudaDeviceSynchronize(); blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock; bitReverseKernel\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(spectrum, N, lgn); cudaDeviceSynchronize(); for(int s = 0; s \u0026lt; lgn; s++) { fft_kernel\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(spectrum, N, s); cudaDeviceSynchronize(); } } } fft_kernel 의 경우 각 스레드가 하나의 butterfly 연산을 수행한다. bitReverseKernel은 입력 데이터를 비트 반전 순서로 재배열한다. solve 함수는 입력 신호가 2의 거듭제곱 크기가 아닌 경우 DFT를 사용하고, 그렇지 않은 경우 FFT를 수행한다.\nreverse bit order 비트반전은 cooley-tukey 알고리즘 구현에서 성능면에서 중요한 역할을 한다. divide - conquer 과정을 생각해보자.\n[x0, x1, x2, x3, x4, x5, x6, x7] 이라는 입력이 있다고하자. 이 입력을 짝수와 홀수로 나누면\n짝수: [x0, x2, x4, x6] 홀수: [x1, x3, x5, x7] 이 된다. 한번더 재귀적으로 반복하면 짝수의 짝수: [x0, x4] 짝수의 홀수: [x2, x6] 홀수의 짝수: [x1, x5] 홀수의 홀수: [x3, x7] 이 된다. 이를 다시 합치면 [x0, x4, x2, x6, x1, x5, x3, x7] 이 순서가 바로 비트 반전 순서이다. 인덱스를 3비트 이진수로 표현하면 다음과 같다 [000, 100, 010, 110, 001, 101, 011, 111] 이고 이를 뒤집으면 [000, 001, 010, 011, 100, 101, 110, 111] 이 된다. 즉 비트 반전 순서로 데이터를 재배열하면 divide - conquer 과정에서의 메모리 접근 패턴이 개선되어 캐시 효율성이 높아지고, 병렬 처리 시 스레드 간의 동기화가 용이해진다. Imporvements shared memory 사용 Algorithm per signal Length 지금까지 살펴본 FFT 알고리즘은 입력 신호의 길이가 2의 거듭제곱일 때 가장 효율적이다. 그러나 실제 신호는 항상 그런것이 아니다. 이럴때는 여러 전략을 필요로 한다.\nZero Padding: 입력 신호의 길이를 가장 가까운 2의 거듭제곱으로 늘리기 위해 0으로 채우는 방법이다. 이 방법은 구현이 간단하고 FFT 알고리즘을 그대로 사용할 수 있지만, 추가된 0이 결과에 영향을 미칠 수 있다.\nMixed-Radix FFT: 입력 신호의 길이가 2의 거듭제곱이 아닐 때도 효율적으로 처리할 수 있는 알고리즘이다. 예를 들어, 길이가 12인 신호는 3과 4의 곱이므로, 3-점과 4-점 FFT를 결합하여 처리할 수 있다. 이 방법은 다양한 길이의 신호에 대해 유연하게 대응할 수 있다.\nBluestein\u0026rsquo;s Algorithm: 임의의 길이의 신호에 대해 FFT를 수행할 수 있는 알고리즘이다. 이 방법은 입력 신호를 특정한 방식으로 변환하여, 길이가 2의 거듭제곱인 신호로 바꾸고, 그 후에 FFT를 적용한다. 이 방법은 모든 길이의 신호에 대해 적용 가능하지만, 구현이 복잡하다.\nRader\u0026rsquo;s Algorithm: 소수 길이의 신호에 대해 FFT를 수행할 수 있는 알고리즘이다. 이 방법은 입력 신호를 순환 컨볼루션으로 변환하여, 길이가 2의 거듭제곱인 신호로 바꾸고, 그 후에 FFT를 적용한다. 이 방법 역시 구현이 복잡하다.\nReference The Introduction to Algorithms, Third Edition Discrete Fourier Transform Cooley–Tukey FFT algorithm Fast Fourier Transform (FFT) Polynomial multiplication "
  },
  {
    "title": "Data_format",
    "permalink": "https://elwhyjay.github.io/post/data_format/",
    "date": "2025-12-28",
    "tags": ["AI","GPU","LLM","CUDA"],
    "content": "현대 AI 데이터 타입: ML 엔지니어와 CUDA 개발자를 위한 기술 심층 분석 저정밀도 수치 포맷은 효율적인 AI 추론의 핵심이 되었으며, FP8은 Hopper GPU에서 2배의 처리량 증가를 제공하면서 FP16에 가까운 정확도를 유지하고, INT4 가중치 전용(weight-only) 양자화는 70B 파라미터 모델을 단일 48GB GPU에서 실행할 수 있게 해준다. 이 보고서는 15개 이상의 데이터 포맷에 대한 수학적 명세, 4세대 NVIDIA 아키텍처 전반에 걸친 하드웨어별 성능 특성, 그리고 양자화 알고리즘에 대한 정량적 분석을 제공한다. 이는 프로덕션 LLM 시스템을 배포하는 데 필수적인 지식이다.\nIEEE 754 기초와 표준 부동소수점의 진화 IEEE 754-2008 표준은 모든 최신 AI 데이터 타입이 구축되거나 수정된 수학적 기반을 정의한다. 정규화된 부동소수점 값의 경우:\n$$v = (-1)^S \\times 2^{(E-\\text{bias})} \\times \\left(1 + \\frac{M}{2^m}\\right)$$\n여기서 S는 부호 비트, E는 저장된 지수(exponent), bias는 지수 오프셋, M은 가수(mantissa), m은 가수 비트 폭이다.\nFP32 (binary32)는 1개의 부호 비트, 8개의 지수 비트(bias=127), 23개의 가수 비트를 사용하여 ±3.4×10³⁸의 다이나믹 레인지와 약 7.22 십진수 자릿수의 정밀도를 제공한다. 머신 엡실론 ε = 2⁻²⁴ ≈ 5.96×10⁻⁸은 1.0에서 표현 가능한 가장 작은 차이를 정의한다.\nFP16 (binary16)은 5개의 지수 비트(bias=15)와 10개의 가수 비트를 포함하여 16비트로 압축된다. 표현 가능한 최대 값은 65,504로 떨어지며, 정밀도는 약 3.31 십진수 자릿수로 낮아진다. 이 포맷은 혼합 정밀도(mixed-precision) 기법이 FP32와 동등한 수렴성을 보여준 후 학습에 널리 사용되게 되었다.\nBF16 (Brain Floating Point)은 근본적으로 다른 접근 방식을 취한다. FP32의 8비트 지수를 유지(±3.4×10³⁸ 다이나믹 레인지 보존)하면서 가수를 7비트로 잘라낸다. 이 설계는 하위 16비트를 버리는 것만으로 FP32에서의 직접적인 절삭(truncation) 변환을 가능하게 하여, 기울기(gradient) 크기가 크게 변하는 학습 워크플로우에 이상적이다.\nFormat Sign Exp Mantissa Bias Max Value Min Normal Precision (ε) FP32 1 8 23 127 3.4×10³⁸ 1.2×10⁻³⁸ 2⁻²⁴ FP16 1 5 10 15 65,504 6.1×10⁻⁵ 2⁻¹¹ BF16 1 8 7 127 3.4×10³⁸ 1.2×10⁻³⁸ 2⁻⁸ TF32 1 8 10 127 3.4×10³⁸ 1.2×10⁻³⁸ 2⁻¹¹ TF32 (TensorFloat-32)는 텐서 코어(Tensor Cores)를 위한 NVIDIA의 실용적인 타협안이다. 총 19비트(8 지수 + 10 가수)로 FP32의 다이나믹 레인지와 FP16 수준의 정밀도를 달성한다. 결정적으로, TF32는 저장 포맷이 아닌 연산 모드이다. 입력은 FP32로 저장되고, 곱셈을 위해 TF32 정밀도로 반올림된 다음, 다시 FP32로 누적된다.\n정밀도와 처리량의 경계를 재정의하는 FP8 포맷 NVIDIA/Intel/ARM 공동 사양은 서로 다른 신경망 연산에 최적화된 두 가지 상호 보완적인 8비트 포맷을 정의한다.\nFP8 E4M3 (4 지수, 3 가수 비트, bias=7)은 좁은 다이나믹 레인지 내에서 정밀도를 극대화한다. 최대 값은 ±448이며 최소 정규 값은 2⁻⁶ ≈ 0.0156이다. 이 명세는 무한대(infinity) 인코딩을 의도적으로 제거하고 E=1111, M=111을 NaN으로 예약하여 IEEE 호환 대안에 비해 사용 가능한 범위를 확장했다.\nFP8 E5M2 (5 지수, 2 가수 비트, bias=15)는 정밀도를 희생하여 FP16 수준의 다이나믹 레인지(±57,344)를 제공한다. 이 포맷은 IEEE 호환 무한대 및 NaN 표현을 유지하므로, 크기 변동성이 정밀도 요구 사항을 초과하는 기울기(gradient)에 적합하다.\n권장 사용 패턴: 순전파(가중치 및 활성화)에는 E4M3, 역전파(기울기)에는 E5M2. 제한된 다이나믹 레인지를 보상하기 위해 텐서별 스케일링 팩터를 사용한다:\n$$x_{\\text{scaled}} = x \\times s_{\\text{tensor}}, \\quad s_{\\text{tensor}} = \\frac{\\max(|x|)}{448}$$\nFormat Max Value Min Normal Precision Special Values Primary Use FP8 E4M3 ±448 0.0156 2⁻⁴ No ∞, limited NaN Forward pass FP8 E5M2 ±57,344 6.1×10⁻⁵ 2⁻³ IEEE-compliant Backward pass 8비트 미만 포맷을 통한 극단적인 압축 INT8 양자화는 아핀 변환(affine transformation)을 사용하여 연속적인 값을 256개의 이산 레벨로 매핑한다:\n$$Q(x) = \\text{round}\\left(\\frac{x}{s}\\right) + z, \\quad s = \\frac{\\max - \\min}{2^8 - 1}$$\nSigned INT8은 [-128, +127] 범위를, Unsigned는 [0, 255] 범위를 커버한다. 양자화 오차는 다음과 같다:\n$$\\text{SQNR} \\approx 6.02b + 1.76 \\text{ dB}$$\n8비트의 경우 약 50 dB의 신호 대 양자화 잡음비(SQNR)를 제공한다.\nINT4는 16개의 이산 레벨(signed: [-8, +7])로 더 압축하며, 보통 패킹되어(바이트당 두 값) 저장된다. 32-128 요소 크기의 블록 단위 양자화(Group-wise quantization)는 그룹별로 별도의 스케일 팩터를 제공하여 균일한 텐서별 양자화에서 손실된 정확도를 복구한다.\nNF4 (Normal Float 4-bit)는 정보 이론적 접근 방식을 취한다. 균일한 간격 대신, 16개의 코드북 값이 표준 정규 분포 하에서 동일한 면적을 생성하도록 배치된다. 값 {-1.0, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911, 0, 0.0796, 0.1609, 0.2461, 0.3379, 0.4407, 0.5626, 0.7230, 1.0}은 정규 분포를 따르는 신경망 가중치에 대한 예상 양자화 오차를 최소화하며, 이는 QLoRA 효율성의 기초가 된다.\nFP4 E2M1 (NVIDIA Blackwell의 기본 포맷)은 bias=1인 2개의 지수 비트와 1개의 가수 비트를 사용하여 {0, 0.5, 1, 1.5, 2, 3, 4, 6} 및 그 음수 값을 생성한다. NVIDIA의 NVFP4 구현은 블록별 FP8 E4M3 스케일링 (16개 요소 블록)과 텐서별 FP32 스케일링을 추가한다:\n$$x = x_q \\times s_{\\text{block}} \\times s_{\\text{tensor}}$$\nOCP Microscaling 포맷: 블록 부동소수점의 표준화 OCP(Open Compute Project)의 MX 사양(AMD, ARM, Intel, Meta, Microsoft, NVIDIA, Qualcomm 승인)은 32개 요소가 단일 E8M0 지수를 공유하는 블록 부동소수점 포맷을 정의한다.\nFormat Element Format Element Bits Block Size Scale Format Effective bits/value MXFP8 E4M3 or E5M2 8 32 E8M0 (8-bit) 8.25 MXFP6 E3M2 or E2M3 6 32 E8M0 (8-bit) 6.25 MXFP4 E2M1 4 32 E8M0 (8-bit) 4.25 MXINT8 INT8 8 32 E8M0 (8-bit) 8.25 E8M0 스케일 포맷은 가수 없이 순수 지수만으로 2⁻¹²⁷에서 2¹²⁷까지의 2의 거듭제곱을 나타낸다. 32개 요소의 MXFP4 블록의 경우: 32×4 + 8 = 136비트, 즉 요소당 4.25비트로 스칼라 FP4 대비 6.25%의 오버헤드만 발생한다.\nMicrosoft Research의 주요 발견: MXFP4 가중치 + MXFP6 활성화를 사용하면 FP32 베이스라인의 0.3% 이내로 학습 수렴을 달성할 수 있다.\nNVIDIA 아키텍처의 발전과 포맷 각 GPU 세대는 텐서 코어 기능과 데이터 타입 지원을 확장한다.\nAmpere (SM80/SM86) - 3세대 텐서 코어 A100은 투명한 FP32 가속기로서 TF32와, 2배의 유효 처리량을 위한 구조적 2:4 희소성(sparsity)을 도입했다. 주요 사양:\nData Type Dense TFLOPS Sparse TFLOPS (2:4) TF32 156 312 BF16/FP16 312 624 INT8 624 TOPS 1,248 TOPS INT4 1,248 TOPS 2,496 TOPS 메모리 대역폭: 2,039 GB/s (HBM2e). WMMA 행렬 모양: FP16의 경우 m16n16k16, TF32의 경우 m16n16k8.\nAda Lovelace (SM89) - 4세대 텐서 코어 L40S와 RTX 4090은 전체 트랜스포머 엔진 없이 FP8 지원을 추가했다. L40S는 733 TFLOPS (FP8 dense), 희소성 적용 시 1,466 TFLOPS를 달성한다. 메모리 대역폭은 864 GB/s(GDDR6)로 떨어져, HBM 시스템에 비해 연산 제한(compute-bound)적인 성격을 띤다.\nHopper (SM90) - 트랜스포머 엔진의 데뷔 H100은 레이어별로 자동 FP8/FP16 정밀도 관리를 제공하는 트랜스포머 엔진과 함께 근본적인 도약을 보여준다.\nData Type Dense TFLOPS Sparse TFLOPS (2:4) TF32 500 1,000 BF16/FP16 1,000 2,000 FP8 2,000 4,000 INT8 2,000 TOPS 4,000 TOPS 메모리 대역폭: 3.35 TB/s (HBM3, H100) ~ 4.8 TB/s (HBM3e, H200). H200은 FP8 처리량을 3,958 TFLOPS(dense)까지 끌어올린다.\nHopper는 스레드 블록 클러스터(SM 간 협력)와 비동기 텐서 데이터 이동을 위한 TMA (Tensor Memory Accelerator)를 도입했다. 이는 이러한 처리량 수준에서 메모리 대기 시간을 숨기는 데 매우 중요하다.\nBlackwell (SM100) - 네이티브 FP4 및 MX 포맷 B200은 FP4를 지원하는 2세대 트랜스포머 엔진으로 성능을 다시 한 번 두 배로 높인다.\nData Type Dense TFLOPS Sparse TFLOPS (2:4) TF32 1,200 2,250 BF16/FP16 2,250 4,500 FP8/FP6 4,500 9,000 FP4 9,000 18,000 메모리 대역폭: 8 TB/s (HBM3e), 용량: 192GB. 10 TB/s NV-HBI 인터커넥트를 갖춘 2080억 트랜지스터 듀얼 다이 설계가 이러한 사양을 가능하게 한다.\n양자화 알고리즘: 정확도와 효율성의 균형 GPTQ: 2차 정보를 활용 GPTQ (Frantar et al., ICLR 2023)는 역헤시안(inverse Hessian) 정보를 사용하여 레이어별 재구성 오차를 최소화한다:\n$$\\arg\\min_{\\hat{W}} |WX - \\hat{W}X|_2^2$$\n헤시안 H = 2XXᵀ + λI는 가중치 섭동(perturbation)에 대한 출력 민감도를 포착한다. 각 양자화된 가중치에 대해:\n$$\\delta_F = -\\frac{[H^{-1}]_{:q}}{[H^{-1}]_{qq}} \\cdot (\\operatorname{quant}(w_q) - w_q)$$ 이 오차 보상은 남아있는 비양자화 가중치로 전파된다. GPTQ의 핵심 혁신: 고정 순서 양자화(열 단위)는 숄레스키 분해를 통해 역 헤세 행렬을 한 번만 계산하게 하여, 레이어당 복잡도를 O(d³)에서 O(d²)로 줄인다.\n성능: 175B 모델을 약 4 GPU 시간 만에 3-4비트로 양자화함. FP16 대비 A100에서 3.25배, A6000에서 4.5배의 속도 향상을 달성.\nAWQ: 활성화 인식 중요 가중치 식별 AWQ (Lin et al., MLSys 2024)는 단 1%의 가중치(크기가 큰 활성화와 정렬된 가중치)를 보호하는 것만으로도 양자화 오차를 획기적으로 줄일 수 있음을 관찰했다:\n$$\\text{Importance}(w_j) \\propto \\mathbb{E}[|X_{:,j}|]$$\nAWQ는 (하드웨어적으로 비효율적인) 혼합 정밀도 대신 동등한 채널별 스케일링을 적용한다:\n$$Y = XW = (X \\cdot \\text{diag}(s)^{-1}) \\cdot (\\text{diag}(s) \\cdot W)$$\n스케일 팩터는 이전 LayerNorm으로 융합(fused)될 수 있어 하드웨어 효율성을 유지하면서 낮은 활성화 채널의 양자화 오차를 \u0026ldquo;용인\u0026quot;한다. AWQ는 모델 크기 전반에 걸쳐 정확도 보존 면에서 GPTQ보다 일관되게 우수하다.\nSmoothQuant: 어려움을 활성화에서 가중치로 이동 LLM 활성화에는 일반적인 값보다 약 100배 큰 이상치(outliers)가 고정된 채널에 집중되어 있다. SmoothQuant (Xiao et al., ICML 2023)는 채널별 스케일링을 도입한다:\n$$s_j = \\frac{\\max(|X_j|)^\\alpha}{\\max(|W_j|)^{1-\\alpha}}$$\n이동 인자(migration factor) α는 난이도 분포를 제어한다. α=0.5는 활성화/가중치 양자화의 균형을 맞추고(OPT, BLOOM에 최적), α=0.75는 심각한 이상치가 있는 모델(GLM-130B)에 적합하다. 이는 INT8 GEMM 하드웨어 가속을 사용하는 W8A8 양자화를 가능하게 하여, 2배의 메모리 감소와 함께 최대 1.56배의 속도 향상을 제공한다.\n메모리 대역폭이 지배하는 LLM 추론 경제학 연산 강도(Arithmetic intensity)(FLOPs/byte)는 추론이 메모리 제한적인지 연산 제한적인지를 결정한다:\n$$\\text{Critical Batch Size} = \\frac{\\text{Accelerator FLOPS/s}}{\\text{Memory Bandwidth}} \\times \\frac{\\operatorname{bits_{param}}}{\\operatorname{bits_{activation}}}$$\nH100에서 BF16 사용 시: B_crit ≈ 280 토큰. 이 임계값 아래에서는 추론이 메모리 제한(memory-bound) 상태이며, 성능은 데이터 전송 감소에 비례하므로 가중치 양자화가 매우 효과적이다.\nKV-캐시 메모리는 시퀀스 길이와 배치 크기에 따라 선형적으로 증가한다:\n$$\\text{KV Size} = 2 \\times L \\times n_{kv} \\times d_h \\times \\text{seq_len} \\times \\text{batch} \\times \\text{bytes}$$\nLlama-2-13B의 경우 BF16에서 시퀀스 길이 8192일 때: 시퀀스당 6.7GB. 4개 시퀀스만 되어도 KV-캐시가 모델 파라미터 메모리를 초과한다.\nKV-캐시 양자화 기법 KIVI (ICML 2024)는 비대칭 분포를 활용한다. Key는 고정 채널 이상치를 가지므로 채널별 양자화를 사용하고, Value는 뚜렷한 패턴이 없으므로 토큰별 양자화를 사용한다. 결과: 2비트 KV-캐시로 2.6배의 메모리 감소와 2.35-3.47배의 처리량 향상 달성.\nHopper에서의 FP8 KV-캐시는 최소한의 정확도 영향으로 2배 압축을 제공한다. NVIDIA는 더 나은 정확도 보존을 위해 KV-캐시에 INT8보다 FP8을 권장한다.\n추측 디코딩(Speculative decoding)을 통한 자기회귀 생성 가속 EAGLE (특징 레벨 추측)은 토큰 대신 두 번째 상위 레이어의 활성화를 예측하여, Medusa의 ~0.6 대비 ~0.8의 초안 정확도(draft accuracy)를 달성한다. 다중 레이어 융합을 사용하는 EAGLE-3는 동일한 출력 분포를 유지하면서 최대 6.5배의 속도 향상(무손실 가속)에 도달한다.\nMethod Draft Accuracy Typical Speedup Architecture Standard Speculative 0.4-0.6 1.5-2× Separate draft model Medusa ~0.6 ~2× Multiple prediction heads EAGLE ~0.8 2-3× Feature-level prediction EAGLE-3 Higher Up to 6.5× Multi-layer fusion 프레임워크 생태계를 통한 실전 배포 TorchAO: PyTorch 네이티브 양자화 TorchAO는 커널 융합을 위해 torch.compile과 직접 통합된다:\nfrom torchao.quantization import quantize_, Int4WeightOnlyConfig quantize_(model, Int4WeightOnlyConfig(group_size=32)) INT4/INT8 가중치 전용, FP8 동적 양자화 및 QAT 워크플로우를 지원한다. 성능: Llama-3-8B에서 INT4 + 2:4 희소성은 67.7%의 메모리 감소와 함께 2.37배의 처리량을 달성한다.\nvLLM 및 SGLang: 서빙 처리량 최적화 vLLM은 효율적인 KV-캐시 관리를 위해 PagedAttention과 함께 GPTQ, AWQ, FP8, INT8을 지원한다. FP8 W8A8은 2배의 메모리 감소와 최대 1.6배의 처리량 향상을 달성한다.\nSGLang은 자동 KV-캐시 접두사 재사용을 위한 RadixAttention을 추가하여, 멀티 턴/퓨샷 워크로드에서 베이스라인 대비 최대 6.4배의 처리량을 제공한다. GPU당 3.56배 더 많은 토큰을 위한 실험적인 FP4 KV-캐시를 지원한다.\nTensorRT-LLM: NVIDIA 하드웨어 활용 극대화 권장 양자화 계층 구조: FP8 (최고의 정확도/성능)로 시작하여, INT8 SmoothQuant로 대체, 메모리 제약이 있을 경우 INT4 AWQ/GPTQ 사용.\nBatch Size Bottleneck Recommended Format ≤4 Memory bandwidth INT4 weight-only (AWQ) 5-15 Mixed FP8 or INT8 SmoothQuant ≥16 Compute + Memory FP8 (W8A8) llama.cpp: 엣지 배포 활성화 K-quants를 사용하는 GGUF 포맷은 품질 인식 양자화를 제공한다:\nType Size (7B) Perplexity Increase Recommendation Q8_0 8.54 GB Minimal Maximum quality Q5_K_M 5.73 GB +0.035 Quality/size balance Q4_K_M 4.92 GB +0.054 Default choice Q3_K_M 4.02 GB +0.244 Memory-constrained 중요도 행렬 (imatrix) 보정은 Q3 이하에서 필수적이며, 가중치 민감도에 따라 비트를 할당하기 위해 활성화 통계를 사용한다.\n정량적 벤치마크를 통한 배포 결정 가이드 정밀도별 처리량 비교 H100에서의 Llama 3-8B:\nFP16: 135.79 tokens/sec (베이스라인) INT8: 158.90 tokens/sec (1.17배) INT4: 211.50 tokens/sec (1.56배) Atom W4A4 양자화: 동등한 메모리에서 FP16 대비 7.73배, INT8 대비 2.53배 처리량.\nMistral 7B FP8 대 FP16: 출력 토큰 33% 향상, TTFT 8.5% 감소, VRAM: 7GB 대 16GB.\n정확도 저하 패턴 2B-405B 모델에 대한 포괄적인 평가 (Jemin Lee et al., 2025):\n모든 양자화 방식은 표준 벤치마크에서 0-2%의 정확도 차이를 보임 가중치 전용 양자화에서 AWQ가 GPTQ보다 일관되게 우수함 더 큰 모델(70B+)이 공격적인 양자화를 더 잘 견딤 (4-bit Llama-2-13B가 FP16 Llama-2-7B를 능가) 작업 민감도는 다양함: 일반 지식 작업보다 TruthfulQA 및 지시 따르기(instruction-following)에서 더 높은 저하를 보임 프로덕션 배포를 위한 메모리 요구 사항 Model FP16 INT8 INT4 (AWQ) Llama 3.1-8B ~16 GB ~8 GB ~4 GB Llama 3.1-70B ~140 GB ~70 GB ~35 GB Llama 3.1-405B ~810 GB ~405 GB ~203 GB 배포 구성: 8B는 단일 A10G/L4에 적합; 70B FP16은 4× A100 또는 2× A100에서 AWQ 필요; 405B FP8은 8× H100 또는 8× A100에서 AWQ 필요.\n결론 AI 수치 포맷의 지형은 단순한 IEEE 754 준수에서 신경망 특성에 최적화된 도메인 특화 타입의 풍부한 생태계로 진화했다. 세 가지 핵심 통찰이 드러난다:\n첫째, 포맷 선택은 메모리/연산 병목 체계에 결정적으로 의존한다. 메모리 대역폭이 지배적인 작은 배치 크기에서는 INT4 가중치 전용이 뛰어나고, 연산 활용도가 중요한 큰 배치에서는 FP8 W8A8이 최적이다.\n둘째, OCP Microscaling 명세는 블록 부동소수점 접근 방식에 대한 업계의 수렴을 보여주며, MXFP4는 공유 지수 메커니즘을 통해 4배의 메모리 감소로 FP32와 동등한 학습 품질을 달성한다.\n셋째, 프로덕션 배포는 하드웨어 기능을 포맷 요구 사항과 일치시켜야 한다. FP8 텐서 코어는 Hopper/Ada(SM89+)를 필요로 하고, FP4 네이티브 지원은 Blackwell(SM100)을 필요로 하며, 최적의 커널 선택은 배치 크기와 시퀀스 길이에 따라 달라진다.\n2025년 NVIDIA 하드웨어를 목표로 하는 실무자를 위한 제언: 최고의 정확도/성능 균형을 위해 Hopper에서 FP8로 시작하고, 메모리 제약 시나리오에는 AWQ INT4를 사용하며, 도구 성숙도에 따라 Blackwell에서의 NVFP4 채택을 준비하라. 다이나믹 레인지 계산, 정밀도 손실 모델링, 양자화 오차 공식과 같은 수학적 기초는 확장되는 포맷 환경에서 정보에 입각한 트레이드오프 결정을 내리기 위한 분석적 프레임워크를 제공한다.\nTHIS ARTICLE IS AUTO GENERATED FROM CLAUDE. "
  },
  {
    "title": "Int4 to FP16 dequantization optimization",
    "permalink": "https://elwhyjay.github.io/post/int8tofloat16/",
    "date": "2025-12-02",
    "tags": ["Quantization","GPU","Optimization","pinned"],
    "content": "Introduction Quantization은 현재 LLM 분야에서 중요한 기술로 자리잡았다. 특히 추론을 최적화하는데 있어서 가중치를 int4, int8 등의 저정밀도 정수형으로 양자화하는 기법이 널리 사용되고 있다. 이 기법을 통해 모델의 메모리 사용량을 줄이고, 메모리 대역폭 요구사항을 낮추며, 하드웨어 가속기의 연산 처리량을 극대화할 수 있다. 하지만 양자화 기술은 만능이 아니고 많은 경우에 최적화가 요구될 수 있다.\n이 글에서는 QServe에서 소개된 Kim et al. 의 논문에서 제시된 int4 양자화된 KV cache를 FP16 활성화 값으로 역양자화하는 비트 최적화 기법에 대해 살펴본다. 역양자화는 때로 과도한 연산량을 요구할 수 있기 때문에, 이를 효율적으로 처리하는 것이 중요하다. 이 최적화 기법은 computing resource를 절약하는 효과적인 방법중 하나로 이해하고 비슷한시나리오에서 사용가능할것 같다.\nFP16 (반정밀도 부동소수점) 기본 구조 FP16은 실수(부동소수점 수)를 표현하는 16비트 포맷이다. 이는 표준 단정밀도(FP32, 32비트) 대비 메모리 효율성 및 연산 처리량 증대에 유리하다.\nFP16은 IEEE 754 표준에 따라 다음과 같이 16비트를 세 부분으로 나누어 구성된다:\n부호 (Sign): 1 비트 지수 (Exponent): 5 비트 (바이어스 $Bias=15$) 가수/유효숫자 (Mantissa/Significand): 10 비트 FP16으로 표현되는 값 $V$의 일반적인 공식은 다음과 같다:\n$$V = (-1)^{Sign} \\times 2^{Exponent - Bias} \\times (1 + Fraction)$$\n암묵적 선행 비트의 역할 유효숫자는 일반적으로 **정규화된 수(Normalized Number)**에서 암묵적인 선행 **$1$**을 가정하여 11비트의 정밀도를 확보한다.\n정규화된 수: 지수 필드가 $0$이나 최대값($31$)이 아닐 때, 선행 비트는 **$1$**로 가정된다. 비정규화된 수 (Denormalized Number): 지수 필드가 **모두 $0$**일 때, 선행 비트는 **$0$**으로 가정된다. 이는 $0$ 주변의 매우 작은 수를 표현하여 점진적인 언더플로우를 가능하게 한다. 정수 변환과 $2^{10}$ 분기점 정수 $N$을 FP16으로 변환할 때, $2^{10}=1024$는 정확도와 표현 간격이 바뀌는 중요한 분기점이다.\n$N \u0026lt; 1024$인 경우: 정규화 시 지수 $E \\le 9$를 갖는다. 가수에 필요한 비트 수가 10비트 이하여서, $1024$ 미만의 모든 정수는 FP16으로 오차 없이 정확하게 표현된다.\n$N \\ge 1024$인 경우: 정규화 시 지수 $E \\ge 10$을 갖는다.\n$N=1027$을 예로 들면, 정규화 형태는 $1.0000000011_2 \\times 2^{10}$이다. $V = (1 + Fraction) \\times 2^{10}$의 형태에서, 가수의 최소 단위 $ULP = 2^{-10}$이 $2^{10}$과 곱해지면서 표현 간격 $\\Delta V$는 $1$이 된다. $$\\Delta V = 2^{-10} \\times 2^{10} = 1$$ 따라서 $1024$ 이상의 정수는 $1$ 간격으로 떨어져 있는 정수만 오차 없이 표현할 수 있다. Int4 $\\to$ FP16 역양자화 최적화 기법 대규모 MoE(Mixture of Experts) 모델의 추론 최적화 과정에서, 4/8비트 정수형 가중치($Int8/Int4$)를 FP16 활성화 값으로 역양자화(Dequantize)하는 과정의 성능 향상을 위해 FP16의 비트 패턴 특징이 활용된다. 이 방법은 느린 네이티브 $Int \\to Float$ 변환(I2F)을 대체한다.\n핵심 관찰 사항 관찰 1: FP16에서 $1024 \u0026lt; X \u0026lt; 2048$ 범위의 정수 $X$는 $1024$가 지수 비트에 표현되고, $int(X-1024)$는 가수 비트에 직접 저장된다.\n관찰 2: $0 \\le Y \u0026lt; 1024$인 정수 $Y$에 대해, $Y+1024$의 FP16 표현은 $1024$의 16진수 표현($0x6400$)에 $Y$를 OR 연산하여 쉽게 만들 수 있다.\nInt4 역양자화 과정 최적화 오프셋 추가: 부호 있는 $Int4$ 가중치에 $\\mathbf{128}$을 더하여 $unsigned$ $Int4$ 값($W_{+}$)을 만든다.\n고속 FP16 생성: $W_{+}$의 값($e_i$)에 $1024$를 더한 형태($e_i + 1024$)의 FP16 비트 패턴을 관찰 2를 통해 비트 연산으로 생성한다.\n오프셋 제거: 생성된 FP16 값은 $1024$ (비트 트릭 오프셋)와 $128$ (부호 변환 오프셋)을 포함하고 있다. 따라서 **총 오프셋 $\\mathbf{1152}$**를 $FP16$ 부동소수점 뺄셈 연산으로 제거하여 원래의 부호 있는 $Int8$ 값을 $FP16$으로 복구한다.\n이러한 최적화는 $Int \\to Float$ 변환을 고속 ALU 및 $FP16$ 명령어로 대체하며, 역양자화 단계를 GEMM 커널에 융합하여 메모리 트래픽 병목 현상을 해소한다.\nInt4 역양자화 커널 구현 예제 다음은 Int4 양자화 가중치를 FP16으로 역양자화하는 CUDA 커널 구현이다. 출처는 다음 github를 참조하라.\ninline __device__ uint4 dequantize_s4_to_fp16x2(uint32_t const\u0026amp; source) { uint4 result; uint32_t* h = reinterpret_cast\u0026lt;uint32_t*\u0026gt;(\u0026amp;result); uint32_t const i4s = reinterpret_cast\u0026lt;uint32_t const\u0026amp;\u0026gt;(source); // First, we extract the i4s and construct an intermediate fp16 number. static constexpr uint32_t immLut = (0xf0 \u0026amp; 0xcc) | 0xaa; static constexpr uint32_t BOTTOM_MASK = 0x000f000f; static constexpr uint32_t TOP_MASK = 0x00f000f0; static constexpr uint32_t I4s_TO_F16s_MAGIC_NUM = 0x64006400; // Note that the entire sequence only requires 1 shift instruction. const uint32_t top_i4s = i4s \u0026gt;\u0026gt; 8; // Extract elt_01 - (i4s \u0026amp; 0x000f000f) | 0x64006400 asm volatile(\u0026#34;lop3.b32 %0, %1, %2, %3, %4;\\n\u0026#34; : \u0026#34;=r\u0026#34;(h[0]) : \u0026#34;r\u0026#34;(i4s), \u0026#34;n\u0026#34;(BOTTOM_MASK), \u0026#34;n\u0026#34;(I4s_TO_F16s_MAGIC_NUM), \u0026#34;n\u0026#34;(immLut)); // Extract elt_23 (i4s \u0026amp; 0x00f000f0) | 0x64006400 asm volatile(\u0026#34;lop3.b32 %0, %1, %2, %3, %4;\\n\u0026#34; : \u0026#34;=r\u0026#34;(h[1]) : \u0026#34;r\u0026#34;(i4s), \u0026#34;n\u0026#34;(TOP_MASK), \u0026#34;n\u0026#34;(I4s_TO_F16s_MAGIC_NUM), \u0026#34;n\u0026#34;(immLut)); // Extract elt_45 (top_i4s \u0026amp; 0x000f000f) | 0x64006400 asm volatile(\u0026#34;lop3.b32 %0, %1, %2, %3, %4;\\n\u0026#34; : \u0026#34;=r\u0026#34;(h[2]) : \u0026#34;r\u0026#34;(top_i4s), \u0026#34;n\u0026#34;(BOTTOM_MASK), \u0026#34;n\u0026#34;(I4s_TO_F16s_MAGIC_NUM), \u0026#34;n\u0026#34;(immLut)); // Extract elt_67 (top_i4s \u0026amp; 0x00f000f0) | 0x64006400 asm volatile(\u0026#34;lop3.b32 %0, %1, %2, %3, %4;\\n\u0026#34; : \u0026#34;=r\u0026#34;(h[3]) : \u0026#34;r\u0026#34;(top_i4s), \u0026#34;n\u0026#34;(TOP_MASK), \u0026#34;n\u0026#34;(I4s_TO_F16s_MAGIC_NUM), \u0026#34;n\u0026#34;(immLut)); // FP16 magic numbers static constexpr uint32_t FP16_TOP_MAGIC_NUM = 0x64006400; // {1024, 1024} static constexpr uint32_t ONE_SIXTEENTH = 0x2c002c00; // {1/16, 1/16} static constexpr uint32_t NEG_64 = 0xd400d400; // {-64, -64} // Finally, we construct the output numbers. asm volatile(\u0026#34;sub.f16x2 %0, %1, %2;\\n\u0026#34; : \u0026#34;=r\u0026#34;(h[0]) : \u0026#34;r\u0026#34;(h[0]), \u0026#34;r\u0026#34;(FP16_TOP_MAGIC_NUM)); asm volatile(\u0026#34;fma.rn.f16x2 %0, %1, %2, %3;\\n\u0026#34; : \u0026#34;=r\u0026#34;(h[1]) : \u0026#34;r\u0026#34;(h[1]), \u0026#34;r\u0026#34;(ONE_SIXTEENTH), \u0026#34;r\u0026#34;(NEG_64)); asm volatile(\u0026#34;sub.f16x2 %0, %1, %2;\\n\u0026#34; : \u0026#34;=r\u0026#34;(h[2]) : \u0026#34;r\u0026#34;(h[2]), \u0026#34;r\u0026#34;(FP16_TOP_MAGIC_NUM)); asm volatile(\u0026#34;fma.rn.f16x2 %0, %1, %2, %3;\\n\u0026#34; : \u0026#34;=r\u0026#34;(h[3]) : \u0026#34;r\u0026#34;(h[3]), \u0026#34;r\u0026#34;(ONE_SIXTEENTH), \u0026#34;r\u0026#34;(NEG_64)); return result; } 구현 분석 이 커널은 다음과 같은 특징을 갖는다:\n$Int4$로 양자화된 8개의 가중치 요소($e_0$ ~ $e_7$)를 담고 있는 단일 $uint32_t$ 변수(source)를 입력받아, $half2$ (32비트 레지스터에 FP16 2개) 형식의 4개 레지스터(uint4 result)에 담긴 $FP16$ 결과로 역양자화한다. 최적화 목표: $Int \\to Float$ 변환 명령어 대신 **비트 연산 (LOP3)**과 **고속 $FP16$ 연산 (SUB.F16X2, FMA.RN.F16X2)**을 사용하여 처리량(throughput)을 높인다. source ($uint32_t$)는 8개의 $Int4$ 가중치 요소($e_0$ ~ $e_7$)를 담고 있으며, $\\mathbf{8}$이 더해져 부호 없는($W_{+}$) 상태이다. 주요 상수 정의 상수 값 의미 (FP16 최적화 관점) $\\text{I4s_TO_F16s_MAGIC_NUM}$ $\\mathbf{0x64006400}$ $half2$로 표현된 $\\mathbf{{1024, 1024}}$의 비트 패턴 (관찰 2) $\\text{BOTTOM_MASK}$ $\\mathbf{0x000f000f}$ $Int4$ 요소 중 하위 비트들을 마스킹하여 추출 $\\text{TOP_MASK}$ $\\mathbf{0x00f000f0}$ $Int4$ 요소 중 상위 비트들을 마스킹하여 추출 $LOP3$ 명령어 분석 asm volatile(\u0026quot;lop3.b32\u0026quot; %0, %1, %2, %3, %4;\\n\u0026quot;)는 $PTX$의 논리 연산자 $LOP3$를 사용하여 다음 연산을 수행한다. 이는 $AND$, $OR$, $NOT$ 등을 조합하는 고성능 단일 명령어이다.\n$$h[i] = (i4s\\quad AND\\quad MASK)\\quad OR\\quad \\mathbf{0x64006400}$$\n마스킹 ($AND$): 입력 $i4s$에서 해당하는 $Int4$ 요소들의 비트만 추출한다.\nOR 연산: 추출된 $Int4$ 값($Y$)에 $\\mathbf{0x64006400}$ (1024)를 $OR$ 연산한다. 이 연산은 논문의 관찰 2를 활용하여 $\\mathbf{{e_i+1024, e_{i+1}+1024}}$의 $FP16$ 비트 패턴을 고속으로 생성한다.\n총 4개의 $LOP3$ 명령어로 8개의 $Int4$ 요소가 $FP16$ 비트 패턴으로 저장된다. 특히 $elt_{23}$과 $elt_{67}$은 시프트 없이 처리되기 때문에, $top_i4s$에 필요한 단 하나의 시프트 명령만 사용된다.\n최종 $FP16$ 값 복구 이제 비트 패턴으로 만들어진 임시 $FP16$ 값에 오프셋 제거 및 스케일링을 수행한다.\n상수 값 의미 $\\text{FP16 _ TOP_ MAGIC_ NUM}$ $\\mathbf{0x64006400}$ $half2$로 표현된 $\\mathbf{{1024, 1024}}$ $\\text{ONE_SIXTEENTH}$ $\\mathbf{0x2c002c00}$ $half2$로 표현된 $\\mathbf{{1/16, 1/16}}$ $\\text{NEG_64}$ $\\mathbf{0xd400d400}$ $half2$로 표현된 $\\mathbf{{-64, -64}}$ $Int4$의 총 오프셋 계산 논문에 따르면, $Int4$는 원래 부호 있는($[-8, 7]$) 값이었다.\n부호 변환 오프셋: $8$을 더해 $unsigned$ $Int4$ ($[0, 15]$)로 만든다. 비트 트릭 오프셋: $1024$를 더해 $FP16$ 비트 패턴을 만든다. 따라서 총 오프셋은 $1024 + 8 = \\mathbf{1032}$이다.\n연산 명령어 분석 코드는 4개의 $half2$ 쌍을 두 종류의 연산으로 처리한다: $elt_{01}, elt_{45}$는 $SUB$, $elt_{23}, elt_{67}$은 $FMA$ (Fused Multiply-Add)이다.\nA. $elt_{01}$ 및 $elt_{45}$ 처리 (SUB):\n// Convert elt_01 asm volatile(\u0026#34;sub.f16x2 %0, %1, %2;\\n\u0026#34; : \u0026#34;=r\u0026#34;(h[0]) : \u0026#34;r\u0026#34;(h[0]), \u0026#34;r\u0026#34;(FP16_TOP_MAGIC_NUM)); $\\text{FP16_TOP_MAGIC_NUM}$은 $\\mathbf{{1024, 1024}}$이다. 이 연산은 $FP16$ 값에서 $1024$를 뺀다. 구현에서는 논문의 $Int4$ 최적화 규칙($1032$를 빼야 함)을 직접 따르지 않고, $1024$만 빼고 있다. 나머지 $\\mathbf{8}$에 대한 처리는 다른 부분(스케일링)에 통합되어 있다. B. $elt_{23}$ 및 $elt_{67}$ 처리 (FMA):\n// Convert elt_23 asm volatile(\u0026#34;fma.rn.f16x2 %0, %1, %2, %3;\\n\u0026#34; : \u0026#34;=r\u0026#34;(h[1]) : \u0026#34;r\u0026#34;(h[1]), \u0026#34;r\u0026#34;(ONE_SIXTEENTH), \u0026#34;r\u0026#34;(NEG_64)); $FMA$ 연산은 $A \\times B + C$ 형태이다. $$result = h[1] \\times \\lbrace 1/16, 1/16 \\rbrace + \\lbrace -64, -64 \\rbrace$$\n$h[1]$ (임시 $FP16$ 값)은 $\\mathbf{{e_{2}+1024, e_{3}+1024}}$를 나타낸다. $e_i$는 $Int4$ 가중치로, 최종 역양자화는 $W_{dq} = e_i \\times S$ (Scale)로 표현된다. 이 $FMA$ 연산은 $Int8$ 역양자화 알고리즘과는 다른 스케일링 기반의 역양자화 공식을 따르는 것이다.: $W_{dq} = IntToFloat(W_{quantized}) \\times Scale$.\n$\\mathbf{ \\lbrace 1/16, 1/16 \\rbrace}$은 스케일 값으로 사용되고, $\\mathbf{\\lbrace -64, -64 \\rbrace}$는 오프셋을 상쇄하는 역할을 한다. 이는 $SUB$ 연산과 달리 전체 스케일링이 $FMA$로 융합되어 처리되는 복잡한 로직을 내포하고 있다.\n결론 FP16 포맷의 구조적 특징, 특히 $2^{10}$을 기준으로 정수가 표현되는 방식과 비트 패턴이 값에 대응되는 방식을 활용하여, 대규모 AI 모델 추론 시 $Int4$ 양자화 가중치의 역양자화 과정을 효율적으로 수행할 수 있다.\nReferences QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving Who Says Elephants Can\u0026rsquo;t Run: Bringing Large Scale MoE Models into Cloud Scale Production IEEE 754 Standard for Floating-Point Arithmetic NVIDIA CUDA Programming Guide - PTX ISA "
  },
  {
    "title": "Gpu_architec",
    "permalink": "https://elwhyjay.github.io/post/gpu_architec/",
    "date": "2025-08-06",
    "tags": ["GPU","CUDA"],
    "content": "Introduction gpu programming을 하기위해서는 gpu architecture에 대한 이해가 필요하다. gpu가 무엇인지는 현재에와서는 모르는 사람이 없을것이다. 때문에 gpu에 대한 소개는 생략하고 gpu architecture를 곧바로 살펴보자\nGPU Hardware Architecture gpu architecture는 크게 다음과 같은 요소로 구성된다.\nStreaming Multiprocessors (SMs): GPU의 핵심 컴퓨팅 유닛으로, 병렬 처리를 수행한다. 각 SM은 여러 개의 CUDA 코어를 포함하고 있으며, 동시에 여러 스레드를 실행할 수 있다. HBM (High Bandwidth Memory): GPU의 글로벌 메모리로, 데이터 전송 속도가 매우 빠르다. HBM은 GPU와 CPU 간의 데이터 전송을 최적화하여 높은 성능을 제공한다. DRAM을 적층해 만든 구조로 CPU에서 DRAM의 역할과 유사하다. L2 Cache: GPU의 L2 캐시는 데이터 접근 속도를 높이기 위해 사용된다. L2 캐시는 GPU와 HBM 사이의 중간 캐시 역할을 하며, 자주 사용되는 데이터를 저장하여 빠른 접근을 가능하게 한다. NVLink: 다른 GPU와의 연결을 위한 인터페이스로 , 다중 GPU 시스템에서 높은 대역폭을 제공한다. NVLink는 GPU 간의 데이터 전송 속도를 향상시켜, 병렬 처리 성능을 극대화한다. Streaming Multiprocessors (SMs) SMs는 GPU의 핵심 컴퓨팅 유닛으로, 병렬 처리를 수행한다. 각 SM은 다음과 같은 구성 요소로 이루어져 있다:\nCUDA Cores: SM 내에서 실제 연산을 수행하는 유닛이다. 하나의 코어는 단일 스레드의 연산을 처리한다. Tensor Cores: 딥러닝 연산을 가속화하기 위해 설계된 유닛으로, 행렬 연산을 효율적으로 처리한다. Tensor Cores는 FP16 및 INT8 데이터 타입을 지원하여 높은 성능을 제공한다. Warp: SM 내에서 실행되는 스레드 그룹으로, 32개의 스레드로 구성된다. 각 워프는 동시에 실행되며, GPU의 병렬 처리 능력을 극대화한다. 워프는 스케줄러에 의해 관리되며, 스레드 간의 동기화를 지원한다. Warp Scheduler: SM 내에서 실행될 스레드를 관리하는 유닛이다. 각 SM은 여러 개의 워프(32개의 스레드로 구성)를 동시에 실행할 수 있다. Registers: 각 CUDA 코어가 사용하는 로컬 메모리로, 빠른 데이터 접근을 가능하게 한다. 각 스레드는 자신의 레지스터를 사용하여 데이터를 저장하고 처리한다. L1 Cache: 각 SM 내에서 사용되는 캐시로, 데이터 접근 속도를 높이기 위해 사용된다. L1 캐시는 Shared Memory와 Registers의 역할을 하며, 자주 사용되는 데이터를 저장하여 빠른 접근을 가능하게 한다. Instruction Cache: SM 내에서 실행되는 명령어를 캐싱하여, 명령어 접근 속도를 높인다. 이 캐시는 자주 사용되는 명령어를 저장하여, 반복적인 연산의 성능을 향상시킨다. Texture Units: GPU에서 텍스처 데이터를 처리하는 유닛으로, 이미지 및 비디오 처리에 사용된다. 텍스처 유닛은 고속의 텍스처 캐시를 사용하여, 이미지 데이터를 효율적으로 처리한다. 이 외에도 다른 구성 요소가 있지만, 위의 요소들이 GPU의 핵심적인 역할을 한다. 자세한 사항은 아래 그림을 참고하자.\n위 그림은 A100의 SM구조로 각 SM은 108개의 CUDA 코어, 4개의 Tensor Cores, 64KB의 L1 캐시 및 96KB의 Shared Memory를 갖추고 있다.\nGPU의 논리적 구조 GPU의 하드웨어 구조를 살펴보았으니 GPU는 논리적으로 다음과 같은 구조를 살펴보자\nGlobal Memory: GPU의 메인 메모리로, 모든 SM이 접근할 수 있는 데이터 저장 공간이다. Global Memory는 대용량 데이터를 저장할 수 있지만, 접근 속도가 상대적으로 느리다. GPU의 Global Memory는 CPU의 DRAM과 유사하다. Grid: GPU에서 실행되는 커널(함수) 실행의 논리적 단위로, 여러 개의 블록으로 구성된다. Block: 각 블록은 여러 개의 스레드로 구성되며, 동일한 Shared Memory를 공유한다. 블록은 GPU의 SM에 할당되어 병렬로 실행된다. Thread: 블록 내에서 실행되는 개별 스레드로, 각 스레드는 자신의 레지스터와 Shared Memory를 사용하여 연산을 수행한다. 각 스레드는 독립적으로 실행되며, 다른 스레드와 동기화가 필요하다. CUDA core 혹은 Tensor core 1개가 하나의 스레드를 처리한다. Shared Memory: 각 블록 내에서 사용되는 빠른 메모리로, 블록 내의 모든 스레드가 공유할 수 있다. Shared Memory는 Global Memory보다 빠른 접근 속도를 제공하며, 블록 내의 스레드 간 데이터 공유를 가능하게 한다. SM내에 L1 캐시와 함께 존재하며, 메모리의 크기는 GPU의 Compute Capability에 따라 다르다. 예를 들어, Compute Capability 7.0인 A100 GPU는 각 SM당 192KB의 shared meomory/ L1 캐시를 갖는다. A100의 경우 Shared memory에 0 KB에서 164KB까지 할당할 수 있다.[^3] Register: 각 스레드가 사용하는 로컬 메모리로, 가장 빠른 데이터 접근 속도를 제공한다. 레지스터는 각 스레드의 상태를 저장하며, 연산에 필요한 데이터를 임시로 저장한다. Local Memory: 각 스레드가 사용하는 메모리로, 레지스터에 저장할 수 없는 큰 데이터나 배열을 저장하는 데 사용된다. Local Memory는 Global Memory보다 느리지만, Shared Memory보다 빠르다. Compute capability 2.0 이상인 GPU의 경우, local memory 데이터는 SM의 L1 캐시와 device L2 캐시에 저장된다. 나중에 살펴볼 자료 H100 이후에 생긴 GPU 아키텍쳐의 특징이있다. 다음에 살펴보자\nDistributed Shared Memory: H100 GPU는 Distributed Shared Memory를 도입하여, 여러 GPU 간의 메모리 공유를 가능하게 한다. 이를 통해 다중 GPU 시스템에서 데이터 전송 속도를 향상시킨다. Thread Block Clustering: 여러 SM에서 실행되는 스레드 블록을 클러스터링하여 제어할수 있게해준다. GPU 데이터 흐름 GPU의 데이터 흐름은 다음 그림과 같다.\nHost, 즉 CPU로 부터 할당받은 데이터를 GPU의 Global Memory, Texture memory, Constant memory에 올린다. 이 메모리의 데이터는 Block내의 Thread가 접근할 수 있다. 각 Block은 SM에 할당되어 병렬로 실행되며, 각 Block 내의 Thread는 Shared Memory를 통해 데이터를 공유한다. 각 Thread는 자신의 레지스터를 사용하여 연산을 수행한다.\nReferences NVIDIA A100 Tensor Core GPU Architecture Washington Univ CSE599W: Spring 2018 Lecture 5 slides NVIDIA CUDA Guide NVIDIA Technical Blog - NVIDIA Ampere Architecture In-Depth NVIDIA Technical Blog - NVIDIA Hopper Architecture In-Depth 별준 블로그 "
  },
  {
    "title": "Loss Function in information geometry",
    "permalink": "https://elwhyjay.github.io/post/loss-function/",
    "date": "2025-06-11",
    "tags": ["NLP","Machine-Learning","Information Geometry"],
    "content": "Introduction 머신러닝에서 loss function, 즉 손실함수는 모델의 예측값과 실제값 사이의 차이를 측정하는 역할을 한다. 이 함수는 모델의 성능을 평가하고, 최적화 과정에서 모델의 파라미터를 조정하는 데 사용된다. 정보기하학에서는 손실함수를 기하학적 관점에서 분석할 수 있다. 이 글에서는 정보기하학에서 손실함수의 역할과 그 기하학적 해석에 대해 살펴본다.\nLoss Function in Information Geometry "
  },
  {
    "title": "cuda basic operation",
    "permalink": "https://elwhyjay.github.io/post/cuda-basic-operation/",
    "date": "2025-03-05",
    "tags": ["CUDA"],
    "content": "CUDA 기본연산 기본적인 함수설명 cudamalloc은 device memory를 할당하는 함수이다. cudaMemcpy은 host와 device간의 데이터를 복사하는 함수이다. cudaFree는 device memory를 해제하는 함수이다. cudaDeviceSynchronize는 device에서 실행중인 모든 kernel이 종료될때까지 기다리는 함수이다. 덧셈연산 #include \u0026lt;cuda_runtime.h\u0026gt; __global__ void vector_add(const float* A, const float* B, float* C, int N) { int idx = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; for(int i =idx;i\u0026lt;N;i+=stride) { C[i] = A[i] + B[i]; } } void solve(const float* A, const float* B, float* C, int N) { float *d_A, *d_B, *d_C; // Allocate device memory cudaMalloc(\u0026amp;d_A, N * sizeof(float)); cudaMalloc(\u0026amp;d_B, N * sizeof(float)); cudaMalloc(\u0026amp;d_C, N * sizeof(float)); // Copy input data from host to device cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice); cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice); // Calculate grid and block dimensions int threadsPerBlock = 256; int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock; // Launch the kernel vector_add\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(d_A, d_B, d_C, N); cudaDeviceSynchronize(); // Copy result back to host cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost); // Free device memory cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); } 곱셈연산 #include \u0026lt;cuda_runtime.h\u0026gt; __global__ void matrix_multiplication_kernel(const float* A, const float* B, float* C, int M, int N, int K) { int col = blockIdx.x * blockDim.x + threadIdx.x; int row = blockIdx.y * blockDim.y + threadIdx.y; if(row \u0026lt; M \u0026amp;\u0026amp; col \u0026lt; K) { float val = 0.0; for(int i =0;i\u0026lt;N;i++) { val += A[(row*N) + i]*B[(K*i)+col]; } C[(row*K)+col] = val; } } // A, B, C are device pointers (i.e. pointers to memory on the GPU) // A,B,C 가 device pointer이므로 cudaMalloc과 cudaMemcpy는 생략한다. void solve(const float* A, const float* B, float* C, int M, int N, int K) { dim3 threadsPerBlock(16, 16); dim3 blocksPerGrid((K + threadsPerBlock.x - 1) / threadsPerBlock.x, (M + threadsPerBlock.y - 1) / threadsPerBlock.y); matrix_multiplication_kernel\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(A, B, C, M, N, K); cudaDeviceSynchronize(); } A matrix의 크기가 MxN이고 B matrix의 크기가 NxK일때, C matrix의 크기는 MxK인 곱셈을 수행한다. 이 구현은 꽤 Naive한 구현이다. 이 구현에서 CUDA Kernel은 DRAM을 사용하여 각 thread가 A와 B의 행렬을 곱하는 방식으로 동작한다. 조금더 개선된 속도를 위해 shared memory를 사용하여 A와 B의 행렬을 곱하는 방식이 있다. 구현은 다음과 같다.\n#include \u0026lt;cuda_runtime.h\u0026gt; #define TILE_SIZE 16 __global__ void matrix_multiplication_kernel_with_shared_memory(const float* A, const float* B, float* C, int M, int N, int K) { __shared__ float A_shared[TILE_SIZE][TILE_SIZE]; __shared__ float B_shared[TILE_SIZE][TILE_SIZE]; int col = blockIdx.x * TILE_SIZE + threadIdx.x; int row = blockIdx.y * TILE_SIZE + threadIdx.y; float val = 0.0; for(int i =0; i \u0026lt; (N + TILE_SIZE - 1) / TILE_SIZE; i++) { if(row \u0026lt; M \u0026amp;\u0026amp; i * TILE_SIZE + threadIdx.x \u0026lt; N) { A_shared[threadIdx.y][threadIdx.x] = A[row * N + i * TILE_SIZE + threadIdx.x]; } else { A_shared[threadIdx.y][threadIdx.x] = 0.0; } if(col \u0026lt; K \u0026amp;\u0026amp; i * TILE_SIZE + threadIdx.y \u0026lt; N) { B_shared[threadIdx.y][threadIdx.x] = B[(i * TILE_SIZE + threadIdx.y) * K + col]; } else { B_shared[threadIdx.y][threadIdx.x] = 0.0; } __syncthreads(); for(int j = 0; j \u0026lt; TILE_SIZE; j++) { val += A_shared[threadIdx.y][j] * B_shared[j][threadIdx.x]; } __syncthreads(); } if(row \u0026lt; M \u0026amp;\u0026amp; col \u0026lt; K) { C[row * K + col] = val; } } 두 가지 동작원리를 그림으로 살펴보자\nnaive matmul shared memory matmul "
  },
  {
    "title": "Matryoshka Representation Learning Review",
    "permalink": "https://elwhyjay.github.io/post/matryoshka-representation-learning-review/",
    "date": "2024-09-27",
    "tags": ["NLP","Machine-Learning","paper_review","pinned"],
    "content": "blog도 개설한겸 논문한편을 리뷰해보고자 한다. 정말 오랜만에 작성하는 리뷰이다. 이 논문의 제목은 Matryoshka Representation Learning 으로 Neurips'22 에 accept된 논문이다.\n학습된 representation (예를 들어 encoder로 embedding된 sentence 등)은 다양한 다운스트림 task에 활용된다. Bert 계열의 encoder의 embedding출력값이 768차원으로 고정되듯 각 다운스트림 task에서 고정된 표현은 부족하거나 과할 수 있다. 이 때 적절한 차원으로의 축소 역시 그 크기를 정하는데 많은 리소스를 사용해야만 한다. 이 논문은 마트료시카 표현 학습 방법(MRL)을 제시함으로써 인코더가 다양한 정보로 표현을 임베딩해 다운스트림 task에 적응하는 방법을 제시한다. MRL을 통해 학습된 표현은 단일모델로 학습된 동일한 차원의 표현만큼 잘 학습한다.\n방법은 간단하다. 그림과 같이 임베딩된 $z$에대해 $O(log(d))$개의 임베딩을 생성한다. 이때 단순히 원본임베딩을 자르기만한다. $z_{1:2},z_{1:4},z_{1:8} \u0026hellip; z_{1:d}$ 의 임베딩들에 대해서 각각 linear classifier를 태우고 독립적으로 학습시킨다. multi-class softmax cross entropy loss func를 이용해 loss를 구하고 모든 loss를 합쳐 모델을 동시에 학습시킨다. 이게 전부이다. MRL-E (Efficient - MRL) 은 linear classifier를 하나로 합친다. 이때 $W \\in \\mathbb{R}^{L \\times d}$ 는 $z$의 크기에 맞춰지고 $W_m = W_{1:m}$ 으로 볼 수 있다. (이 방법이 정말 마트료시카랑 비슷한듯). 추가로 adpative classification, retrieval 같은 방법을 사용해 동적으로 표현 크기를 조절 가능하다고 한다.\n*Figure 1 *\n논문은 주로 비젼 task에대해서 실험했음으로 생략하고 이를 NLP 에 활용한 후속 논문인 2D Matryoshka Sentence Embedding 논문을 대충 살펴본다.\n이 논문은 특히 transformer의 모든 layer에 대해 MRL을 적용한다. 학습시에 MRL과 동일하게 마지막 임베딩을 학습하고, 추가로 무작위로 중간 레이어를 선택해 MRL을 적용한다. 또 두 임베딩 loss사이의 KL divergence를 적용해 align 을한다(분포차이를 작게함). 이렇게 전체 loss를 더해 총 loss로 모델을 학습하게된다. 논문에서는 MRL처럼 모든 trunc size를 학습하는게아니라 선택된 크기로 잘라서 그 차원의 임베딩만 학습한다. 또한 여기서 필요에 따라 얕은 레이어의 임베딩만 사용함으로써 inference 속도를 빠르게 할 수 있다.\nFigure 2\nSentence Transformer 사용법 sentence library에 구현이 되어있다. 사용법을 document에서 가져와봤다. 학습의 경우 간단히 loss를 설정해주면된다.\nfrom sentence_transformers import SentenceTransformer from sentence_transformers.losses import CoSENTLoss, MatryoshkaLoss model = SentenceTransformer(\u0026#34;microsoft/mpnet-base\u0026#34;) base_loss = CoSENTLoss(model=model) loss = MatryoshkaLoss(model=model, loss=base_loss, matryoshka_dims=[768, 512, 256, 128, 64]) loss = Matryoshka2dLoss(model=model, loss=base_loss, matryoshka_dims=[768, 512, 256, 128, 64]) inference는 다음과 같다.\nfrom sentence_transformers import SentenceTransformer import torch.nn.functional as F matryoshka_dim = 64 model = SentenceTransformer( \u0026#34;nomic-ai/nomic-embed-text-v1.5\u0026#34;, trust_remote_code=True, truncate_dim=matryoshka_dim, ) embeddings = model.encode( [ \u0026#34;search_query: What is TSNE?\u0026#34;, \u0026#34;search_document: t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map.\u0026#34;, \u0026#34;search_document: Amelia Mary Earhart was an American aviation pioneer and writer.\u0026#34;, ] ) assert embeddings.shape[-1] == matryoshka_dim similarities = model.similarity(embeddings[0], embeddings[1:]) # =\u0026gt; tensor([[0.7839, 0.4933]]) 작은 사이즈의 임베딩 크기를 설정하여도 search_query 에대해서 무관한 문서보다 유관한 문서의 유사성이 높게 나타났다.\nGPT-4 turobo에도 embedding 방법을 활용하는듯 하다.^\nReference Kusupati, Aditya, et al. \u0026ldquo;Matryoshka representation learning.\u0026rdquo; Advances in Neural Information Processing Systems 35 (2022): 30233-30249. https://arxiv.org/pdf/2205.13147v4 https://github.com/RAIVNLab/MRL https://huggingface.co/blog/matryoshka Li, Xianming, et al. \u0026ldquo;2d matryoshka sentence embeddings.\u0026rdquo; arXiv preprint arXiv:2402.14776 (2024).https://arxiv.org/pdf/2402.14776v1 https://sbert.net/examples/training/matryoshka/README.html https://velog.io/@hoon_lander/posts https://openai.com/index/new-embedding-models-and-api-updates/ "
  },
  {
    "title": "STS Project 회고",
    "permalink": "https://elwhyjay.github.io/post/sts-project-%ED%9A%8C%EA%B3%A0/",
    "date": "2024-09-25",
    "tags": ["NLP","Machine-Learning"],
    "content": "프로젝트를 마치며 부스트캠프에서 첫 팀 프로젝트로 캐글형식의 대회를 진행했다. 회고를 쓴 시점에서 최종순위가 나온것은아니지만 대략적인 얼개는 나온것같다. 결론부터 말하면 중간정도의 성적이다. 애당초 시작할때 높은 순위가 목표도 아니었고 또 지속적으로 해온 대회라 데이터의 누적으로 순위가 사소한 차이로 갈렸기 때문에 큰 의미는 두지 않아도 될것 같다. 하지만 그렇다고 아쉬움이 없었던것은 아니다. 추석 연휴가 끼어있어서 상당히 길게 프로젝트를 진행했음에도 미숙하게 한점이 많은것같다. 이는 대체로 나의 잘못인데 밑에서 자세히 적도록 하겠다. 대회는 STS(sematical text similarity) 문제로 두 문장의 유사도를 점수로 나타내고 이를 피어슨 상관계수로 평가하는 형식이었다. 기본적으로 base 코드도 주어지기때문에 큰 틀은 여느 팀이나 비슷했다. 그래서 대회중에 색다른 시도를 몇가지 해보고자 했는데 그에대해 먼저 적어볼까 한다.\nLoss function의 설계 내가 대학원에 있을때, 리소스도 빈약하고 또 아이디어도 특출난게 없으면 만지는것이 손실함수의 설계였다. 아무래도 가성비가 좋다랄까. 특히 비전쪽은 더욱 그러한면이 있었었다. 당연히 근래의 연구트렌드는 이와 먼것이 사실이다. 특히 자연어 처리는 더욱 그럴것이다. 그 이유는 당연 LLM의 등장으로 모델크기와 데이터의 양이 더 중요해져서 이기도하고 이 때문에 학습시간은 더욱 길어져 새로운 손실함수를 설계하고 검증하는일은 성능향상폭에 비해 가성비가 나쁘기 때문이다. 그래도 이번 task는 학습시간이 길지도 않고 기존에 학습되어있는 모델을 새로운 데이터로 fine-tuning하는것이기 때문에 시도해볼만하다고 생각해서 새로운 손실함수를 설계했다.\n설계한 손실함수는 다음과 같다.\n$$\\mathcal{L}_{cor} = 1 - \\rho(y, \\hat{y})$$\n$$\\rho(y, \\hat{y}) = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(\\hat{y}_i - \\bar{\\hat{y}})}{\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} \\sqrt{\\sum_{i=1}^{n} (\\hat{y}_i - \\bar{\\hat{y}})^2}}$$\n즉 상관계수 손실함수는 타겟데이터와 예측데이터의 피어슨 상관계수를 구하고 그 값을 1에서 뺀것으로 정의했다. (손실값을 작게 학습해야함으로) 그리고 이것만으로는 선형관계 파악에만 학습이 치중될거같다는 가정을하고 $ L_1 $ 손실함수와 가중합을 더해 최종 손실함수를 정의 했다.\n$$ \\mathcal{L}_{total} = (1-\\alpha)\\mathcal{L}_{cor} + \\alpha\\mathcal{L}_{MAE} $$\n이렇게 설계한 이유는 우리가 원하는게 정확한 데이터의 값이 아니라 두 데이터의 상관관계가 선형이 되길 원하기 때문이고 이렇게 설정한 손실함수가 변수들간의 관계를 더 잘 학습할 수 있을거라고 (즉 선형성 파악에 유리) 생각했기 때문이다. 다양한 $\\alpha$ 값으로 훈련을 진행해보았는데 결과적으로 $\\alpha$ 값이 작을 수록 성능이 좋았고 0.1 정도로 주었을때 사용하지 않을때랑 성능이 비슷했다. 정확히 따지면 학습시에는 모두 성능이 사용하지않는것보다 우수했는데 제출 데이터는 그렇지 않았던 것이다. 이는 과적합이 발생했다고 해석했는데 과적합이 발생한 이유는학습 데이터를 기반으로 선형성을 학습하게 되는데 과연 언어 데이터가 정말 선형적인가 하는 문제이다. 정확히는 언어의 유사도 변수들이 서로 선형적인 관계인가 하는 문제이다. 학습데이터를 공개할 수 없기때문에 적절히 바꾸어서 예를 들어보겠다.\nsentence 1 sentence 2 score 나는집에 간다 나는 집에간다 5.0 너는 사과를 먹는다 너는 사과를먹는다 4.8 그는 시계를 샀다 그는시계를샀다 4.6 (📍 저작권상 내가 문장을 적절히 만든것이고 실제 데이터도 비슷한 양상정도라고만 이해해주길 바란다.)\n위 데이터에서 첫번째 문장쌍과 두번째문장쌍, 그리고 세번쨰 문장쌍에서 차이점은 띄어쓰기 위치, 그리고 횟수 정도이다. 그런데 이로부터 점수의 선형적인 차가 곧 의미의 차라고 볼 수 있을까? 나는 이런점에 있어서 설계한 손실함수가 적절하지 않았던게 아닌가 싶다.\n이 설계의 아쉬웠던점은 논리적이고 수학적인 가설과 검증을 했다기보다는 감각적으로 수행했다는 점이다. 통계적인 background 부족과 경험 부족 때문이 아닐까 싶지만 구현도 실험도 그렇게 많은 노력이 들지 않았기 때문에 크게 아쉬운 부분은 아니다.\nMulti-task learning 설계 다음으로 시도했던 색다른 시도는 multi task learning을 시도했다. 학습데이터의 레이블은 두개가 주어졌는데, 비슷한 정도를 나타내는 \u0026rsquo;label\u0026rsquo; 과 비슷한지 아닌지를 이분법적으로 구분하는 \u0026lsquo;binary-label\u0026rsquo;이 있었다. base코드에는 후자의 레이블을 전혀 사용하지 않았는데 이를 사용하고자 mult-task learning 을 구현했고 설계는 다음과 같다.\n하지만 이 역시 크게 개선된 효과는 보이지 못했다. 장점이 아주 없던것은 아니었는데 학습의 수렴속도는 제법빨라져서 적은 epoch수로도 비슷한 점수를 달성했다.\n협업에대해 아번 프로젝트에서 가장 아쉬웠던점은 나의 태도가 아닐까 싶다. 우리팀에서 아무래도 내가 가장 연장자였고 경험도 많았다. 하지만 내가 나서서 업무를 조정하거나 일정 조율을 하지는 않고 특별히 이런걸 정하지 않고 각자해서 합치자 이런식으로 진행하게 되었는데 이게 실수였다. 결과적으로 내코드만으로 결과물을 내고 제출하게 되었다. 팀원들이 경험이 있는 친구도 있었고 없는 친구도 있었는데 없는 친구들에 대한 배려가 부족했던것이다. 이 친구들은 어떤식으로 갈피를 잡아야할지 몰라서 사실상 적극적으로 프로젝트에 참여하기 어려웠는데 이런 팀원들이 참여할수있게끔 역할을 했어야 했다. 대체로 또래집단에서는 나서기보다는 내 할것만 충실히했고 한편으로는 나서서 일을 분배하고 조정하는 일을 주제넘는 일이라고 생각한 부분이 있었던것 같다. 그런데 이번경우 내위치에서는 그렇게 하지않는것이 오히려 책임회피가 아니었나 싶다. 이번 프로젝트는 혼자서 감당할만한 수준이었지만 규모가 더 크고 혼자 개발하기 어려운 과제였다면 분명 파행이 났을거라고 생각한다. 협업할때 있어서 주도적으로 할 수 있는 위치에 있는 사람이 주도적으로 진행해야만 하는게 책임감 있는거라는 생각이 들었고 다음번에는 같은 실수를 하지않도록 다짐하면서 회고를 마친다.\n"
  },
  {
    "title": "Hello_blog",
    "permalink": "https://elwhyjay.github.io/post/hello_blog/",
    "date": "2024-09-24",
    "tags": [],
    "content": "This is first blog post "
  },
  {
    "title": "공사장의 구석",
    "permalink": "https://elwhyjay.github.io/post/game-of-life/",
    "date": "2024-09-24",
    "tags": [],
    "content": " Pause Reset alive: 0 gen: 0 click to toggle cells\n"
  },
  {
    "title": "Hello World",
    "permalink": "https://elwhyjay.github.io/post/hello_world/",
    "date": "2024-09-24",
    "tags": [],
    "content": " Torus\nCube\nTetrahedron\nSphere\n"
  }
]
