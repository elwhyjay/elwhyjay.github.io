<!DOCTYPE html>

<script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><style>
    .theme-toggle {
      position: fixed;
      top: 20px;
      right: 20px;
      padding: 8px;
      font-size: 1.2rem;
      cursor: pointer;
      background: none;
      border: 1px solid ;
      border-color: black;
      border-radius: 60%;
      z-index: 1000;
    }
    body[a="dark"] .theme-toggle {
        border-color: white;
    }
</style>

<html lang="ko-kr"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="google-site-verification" content="CGHN0W7w6iWZ8ggs2E8eQ_Wiq1IuSxJHXr-KLZIetgk" />
   <meta name="description" content="blog도 개설한겸 논문한편을 리뷰해보고자 한다. 정말 오랜만에 작성하는 리뷰이다. 이 논문의 제목은
Matryoshka Representation Learning 으로 Neurips&#39;22 에 accept된 논문이다.
학습된 representation (예를 들어 encoder로 embedding된 sentence 등)은 다양한 다운스트림 task에 활용된다. Bert 계열의 encoder의 embedding출력값이 768차원으로 고정되듯 각 다운스트림 task에서 고정된 표현은 부족하거나 과할 수 있다. 이 때 적절한 차원으로의 축소 역시 그 크기를 정하는데 많은 리소스를 사용해야만 한다. 이 논문은 마트료시카 표현 학습 방법(MRL)을 제시함으로써 인코더가 다양한 정보로 표현을 임베딩해 다운스트림 task에 적응하는 방법을 제시한다. MRL을 통해 학습된 표현은 단일모델로 학습된 동일한 차원의 표현만큼 잘 학습한다.">  
  

  <title>
    
      Matryoshka Representation Learning Review
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/img/favicon.ico" />
  
  
  
  <link rel="stylesheet" href="/css/main.b27fc5610be205f24ca50ff0ec7c3029a919c277b959db7ed8b6e173d918decfc1435681e9a787476bce9650012878ff0a4ac9f8ac6f8e2ee2716f8eff5cead8.css" integrity="sha512-sn/FYQviBfJMpQ/w7HwwKakZwne5Wdt&#43;2Lbhc9kY3s/BQ1aB6aeHR2vOllABKHj/CkrJ&#43;Kxvji7icW&#43;O/1zq2A==" />
   <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
 
</head>
<style>
        table {
            border-collapse: collapse;
            border: 1px solid #ebebeb;
        }
        
        th, td {
            border: 1px solid #ffffff;
            padding: 8px;
        }
        th {
            background-color: #2290e5;
        }
        
    </style>
    
    <body a="auto">
        <button class="theme-toggle" onclick="toggleTheme()">
            <span class="light-icon">🌞</span>
            <span class="dark-icon" style="display:none">🌙</span>
        </button>
        <main class="page-content" aria-label="Content">
            <div class="w">
<a href="/">..</a>


<article>
    <p class="post-meta">
        <time datetime="2024-09-27 00:00:29 &#43;0900 KST">
            2024-09-27
        </time>
    </p>

    <h1>Matryoshka Representation Learning Review</h1>

    

    <p>blog도 개설한겸 논문한편을 리뷰해보고자 한다. 정말 오랜만에 작성하는 리뷰이다. 이 논문의 제목은
Matryoshka Representation Learning 으로 Neurips'22 에 accept된 논문이다.</p>
<p>학습된 representation (예를 들어 encoder로 embedding된 sentence 등)은 다양한 다운스트림 task에 활용된다. Bert 계열의 encoder의 embedding출력값이 768차원으로 고정되듯 각 다운스트림 task에서 고정된 표현은 부족하거나 과할 수 있다. 이 때 적절한 차원으로의 축소 역시 그 크기를 정하는데 많은 리소스를 사용해야만 한다. 이 논문은 마트료시카 표현 학습 방법(MRL)을 제시함으로써 인코더가 다양한 정보로 표현을 임베딩해 다운스트림 task에 적응하는 방법을 제시한다. MRL을 통해 학습된 표현은 단일모델로 학습된 동일한 차원의 표현만큼 잘 학습한다.</p>
<p>방법은 간단하다. 그림과 같이 임베딩된 $z$에대해 $O(log(d))$개의 임베딩을 생성한다. 이때 단순히 원본임베딩을 자르기만한다. $z_{1:2},z_{1:4},z_{1:8} &hellip;
z_{1:d}$ 의 임베딩들에 대해서 각각 linear classifier를 태우고 독립적으로 학습시킨다. multi-class softmax cross entropy loss func를 이용해 loss를 구하고 모든 loss를 합쳐 모델을 동시에 학습시킨다. 이게 전부이다. MRL-E (<em>Efficient - MRL</em>) 은 linear classifier를 하나로 합친다. 이때 $W \in \mathbb{R}^{L \times d}$ 는 $z$의 크기에 맞춰지고 $W_m = W_{1:m}$ 으로 볼 수 있다. (이 방법이 정말 마트료시카랑 비슷한듯). 추가로 adpative classification, retrieval 같은 방법을 사용해 동적으로 표현 크기를 조절 가능하다고 한다.</p>
<p><img src="/img/Maytyoshka_img1.png" alt="figure1">*Figure 1 *</p>
<p>논문은 주로 비젼 task에대해서 실험했음으로 생략하고 이를 NLP 에 활용한 후속 논문인 <strong><a href="https://arxiv.org/pdf/2402.14776v1">2D Matryoshka Sentence Embedding</a></strong> 논문을 대충 살펴본다.</p>
<p>이 논문은 특히 transformer의 모든 layer에 대해 MRL을 적용한다. 학습시에 MRL과 동일하게 마지막 임베딩을 학습하고, 추가로 무작위로 중간 레이어를 선택해 MRL을 적용한다. 또 두 임베딩 loss사이의 KL divergence를 적용해 align 을한다(분포차이를 작게함). 이렇게 전체 loss를 더해 총 loss로 모델을 학습하게된다. 논문에서는 MRL처럼 모든 trunc size를 학습하는게아니라 선택된 크기로 잘라서 그 차원의 임베딩만 학습한다. 또한 여기서 필요에 따라 얕은 레이어의 임베딩만 사용함으로써 inference 속도를 빠르게 할 수 있다.</p>
<p><img src="/img/Maytyoshka_img2.png" alt="figure2"><em>Figure 2</em></p>
<h3 id="sentence-transformer-사용법">Sentence Transformer 사용법</h3>
<p>sentence library에 구현이 되어있다. 사용법을 document에서 가져와봤다.
학습의 경우 간단히 loss를 설정해주면된다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sentence_transformers <span style="color:#f92672">import</span> SentenceTransformer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sentence_transformers.losses <span style="color:#f92672">import</span> CoSENTLoss, MatryoshkaLoss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> SentenceTransformer(<span style="color:#e6db74">&#34;microsoft/mpnet-base&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>base_loss <span style="color:#f92672">=</span> CoSENTLoss(model<span style="color:#f92672">=</span>model)
</span></span><span style="display:flex;"><span>loss <span style="color:#f92672">=</span> MatryoshkaLoss(model<span style="color:#f92672">=</span>model, loss<span style="color:#f92672">=</span>base_loss,
</span></span><span style="display:flex;"><span>                    matryoshka_dims<span style="color:#f92672">=</span>[<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">64</span>])
</span></span><span style="display:flex;"><span>loss <span style="color:#f92672">=</span> Matryoshka2dLoss(model<span style="color:#f92672">=</span>model, loss<span style="color:#f92672">=</span>base_loss,
</span></span><span style="display:flex;"><span>                    matryoshka_dims<span style="color:#f92672">=</span>[<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">64</span>])
</span></span></code></pre></div><p>inference는 다음과 같다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sentence_transformers <span style="color:#f92672">import</span> SentenceTransformer
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>matryoshka_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> SentenceTransformer(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;nomic-ai/nomic-embed-text-v1.5&#34;</span>,
</span></span><span style="display:flex;"><span>    trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    truncate_dim<span style="color:#f92672">=</span>matryoshka_dim,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>embeddings <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>encode(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;search_query: What is TSNE?&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;search_document: t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map.&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;search_document: Amelia Mary Earhart was an American aviation pioneer and writer.&#34;</span>,
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">assert</span> embeddings<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">==</span> matryoshka_dim
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>similarities <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>similarity(embeddings[<span style="color:#ae81ff">0</span>], embeddings[<span style="color:#ae81ff">1</span>:])
</span></span><span style="display:flex;"><span><span style="color:#75715e"># =&gt; tensor([[0.7839, 0.4933]])</span>
</span></span></code></pre></div><p>작은 사이즈의 임베딩 크기를 설정하여도 search_query 에대해서 무관한 문서보다 유관한 문서의 유사성이 높게 나타났다.</p>
<p>GPT-4 turobo에도 embedding 방법을 활용하는듯 하다.<a href="https://openai.com/index/new-embedding-models-and-api-updates/">^</a></p>
<hr>
<h1 id="reference">Reference</h1>
<ul>
<li>Kusupati, Aditya, et al. &ldquo;Matryoshka representation learning.&rdquo; Advances in Neural Information Processing Systems 35 (2022): 30233-30249. <a href="https://arxiv.org/pdf/2205.13147v4">https://arxiv.org/pdf/2205.13147v4</a></li>
<li><a href="https://github.com/RAIVNLab/MRL">https://github.com/RAIVNLab/MRL</a></li>
<li><a href="https://huggingface.co/blog/matryoshka">https://huggingface.co/blog/matryoshka</a></li>
<li>Li, Xianming, et al. &ldquo;2d matryoshka sentence embeddings.&rdquo; arXiv preprint arXiv:2402.14776 (2024).https://arxiv.org/pdf/2402.14776v1</li>
<li><a href="https://sbert.net/examples/training/matryoshka/README.html">https://sbert.net/examples/training/matryoshka/README.html</a></li>
<li><a href="https://velog.io/@hoon_lander/posts">https://velog.io/@hoon_lander/posts</a></li>
<li><a href="https://openai.com/index/new-embedding-models-and-api-updates/">https://openai.com/index/new-embedding-models-and-api-updates/</a></li>
</ul>
<hr>

</article>


    <script src="https://utteranc.es/client.js"
        repo="elwhyjay/blog-comment"
        issue-term="title"
        label="✨💬✨"
        theme="github-dark"
        crossorigin="anonymous"
        async>
</script>


            </div>
        </main>
        <script>
            function toggleTheme() {
              const body = document.querySelector('body');
              const currentTheme = body.getAttribute('a');
              const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
              
              body.setAttribute('a', newTheme);
              localStorage.setItem('theme', newTheme);
              
              
              const lightIcon = document.querySelector('.light-icon');
              const darkIcon = document.querySelector('.dark-icon');
              lightIcon.style.display = newTheme === 'dark' ? 'none' : 'inline';
              darkIcon.style.display = newTheme === 'dark' ? 'inline' : 'none';
            }
            
            
            document.addEventListener('DOMContentLoaded', () => {
              const savedTheme = localStorage.getItem('theme');
              if (savedTheme) {
                document.querySelector('body').setAttribute('a', savedTheme);
                
                const lightIcon = document.querySelector('.light-icon');
                const darkIcon = document.querySelector('.dark-icon');
                lightIcon.style.display = savedTheme === 'dark' ? 'none' : 'inline';
                darkIcon.style.display = savedTheme === 'dark' ? 'inline' : 'none';
              }
            });
        </script>
    </body>
</html>
