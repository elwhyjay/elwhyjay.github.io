<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on Yongjun&#39;s blog</title>
    <link>http://localhost:1313/tags/llm/</link>
    <description>Recent content in LLM on Yongjun&#39;s blog</description>
    <generator>Hugo</generator>
    <language>ko-kr</language>
    <lastBuildDate>Sun, 28 Dec 2025 21:59:14 +0900</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Data_format</title>
      <link>http://localhost:1313/post/data_format/</link>
      <pubDate>Sun, 28 Dec 2025 21:59:14 +0900</pubDate>
      <guid>http://localhost:1313/post/data_format/</guid>
      <description>&lt;h1 id=&#34;현대-ai-데이터-타입-ml-엔지니어와-cuda-개발자를-위한-기술-심층-분석&#34;&gt;현대 AI 데이터 타입: ML 엔지니어와 CUDA 개발자를 위한 기술 심층 분석&lt;/h1&gt;&#xA;&lt;p&gt;저정밀도 수치 포맷은 효율적인 AI 추론의 핵심이 되었으며, &lt;strong&gt;FP8&lt;/strong&gt;은 Hopper GPU에서 &lt;strong&gt;2배의 처리량 증가&lt;/strong&gt;를 제공하면서 FP16에 가까운 정확도를 유지하고, &lt;strong&gt;INT4 가중치 전용(weight-only) 양자화&lt;/strong&gt;는 70B 파라미터 모델을 단일 48GB GPU에서 실행할 수 있게 해준다. 이 보고서는 15개 이상의 데이터 포맷에 대한 수학적 명세, 4세대 NVIDIA 아키텍처 전반에 걸친 하드웨어별 성능 특성, 그리고 양자화 알고리즘에 대한 정량적 분석을 제공한다. 이는 프로덕션 LLM 시스템을 배포하는 데 필수적인 지식이다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Llm_error</title>
      <link>http://localhost:1313/post/llm_error/</link>
      <pubDate>Fri, 01 Nov 2024 19:56:05 +0900</pubDate>
      <guid>http://localhost:1313/post/llm_error/</guid>
      <description>&lt;p&gt;LLM 을 다루다 만나는 error를 모아보겠다&lt;/p&gt;&#xA;&lt;h3 id=&#34;error-1-tokenizer-template&#34;&gt;Error 1. tokenizer template&amp;hellip;.&lt;/h3&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ValueError: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;이런 에러는 template이 없을때 발생하는데. 이때는 tokenizer.chat_template을 설정해주면 된다. 주로 mistral,llama2에서 발생한다.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;chat_template &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;...&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;더 자세한 chat_template사용방법은 &lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/chat_templating&#34;&gt;여기&lt;/a&gt;를 참고하자.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
